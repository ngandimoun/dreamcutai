import { getFalClient } from "./fal-client"

// Model types
export type FalModel = "Nano-banana" | "gpt-image-1" | "seedream-v4"

// Aspect ratio mapping for different models
export const ASPECT_RATIO_MAPPING = {
  "Nano-banana": {
    "1:1": "1:1",
    "16:9": "16:9", 
    "9:16": "9:16",
    "4:3": "4:3",
    "3:4": "3:4",
    "21:9": "21:9",
    "2:3": "2:3",
    "4:5": "4:5"
  },
  "gpt-image-1": {
    "1:1": "1024x1024",
    "16:9": "1536x1024",
    "9:16": "1024x1536",
    "4:3": "1024x1024", // Default to square for unsupported ratios
    "3:4": "1024x1536",
    "21:9": "1536x1024", // Default to landscape
    "2:3": "1024x1536",
    "4:5": "1024x1536"
  },
  "seedream-v4": {
    "1:1": { width: 1024, height: 1024 },
    "16:9": { width: 1920, height: 1080 },
    "9:16": { width: 1080, height: 1920 },
    "4:3": { width: 1024, height: 768 },
    "3:4": { width: 768, height: 1024 },
    "21:9": { width: 1920, height: 822 },
    "2:3": { width: 1024, height: 1536 },
    "4:5": { width: 1024, height: 1280 }
  }
}

// Generation parameters interface
export interface GenerationParams {
  prompt: string
  aspectRatio: string
  numImages?: number
  model: FalModel
  hasImages: boolean
  imageUrls: string[]
  logoImageUrl?: string
}

// Generation result interface
export interface GenerationResult {
  success: boolean
  images: string[]
  requestId?: string
  error?: string
}

/**
 * Generate images using fal.ai with intelligent endpoint selection
 */
export async function generateWithFal(params: GenerationParams): Promise<GenerationResult> {
  try {
    const { model, prompt, aspectRatio, numImages = 1, hasImages, imageUrls, logoImageUrl } = params

    // Validate OpenAI API key for gpt-image-1
    if (model === "gpt-image-1" && !process.env.OPENAI_API_KEY) {
      throw new Error("OPENAI_API_KEY environment variable is required for gpt-image-1 model")
    }

    // Build image URLs array - include logo if present
    let finalImageUrls = [...imageUrls]
    
    if (logoImageUrl) {
      // Add logo as second image for GPT Image 1 to reference
      finalImageUrls.push(logoImageUrl)
      console.log('ðŸ“ Including logo image in image_urls for GPT Image 1')
    }

    // Enhance prompt to instruct logo placement
    let enhancedPrompt = prompt
    if (logoImageUrl) {
      enhancedPrompt = `${prompt}. Use the logo from the second reference image and place it on the chart as specified in the prompt.`
    }

    // Validate model
    if (!ASPECT_RATIO_MAPPING[model]) {
      throw new Error(`Unsupported model: ${model}. Supported models are: ${Object.keys(ASPECT_RATIO_MAPPING).join(', ')}`)
    }

    // Validate aspect ratio
    if (!ASPECT_RATIO_MAPPING[model][aspectRatio as keyof typeof ASPECT_RATIO_MAPPING[typeof model]]) {
      throw new Error(`Unsupported aspect ratio '${aspectRatio}' for model '${model}'. Supported ratios are: ${Object.keys(ASPECT_RATIO_MAPPING[model]).join(', ')}`)
    }

    // Map aspect ratio to model-specific format
    const mappedAspectRatio = ASPECT_RATIO_MAPPING[model][aspectRatio as keyof typeof ASPECT_RATIO_MAPPING[typeof model]]

    let result: any

    if (hasImages) {
      // Use edit endpoints when images are present
      const editParams = {
        prompt: enhancedPrompt,
        imageUrls: finalImageUrls,
        aspectRatio: mappedAspectRatio,
        numImages
      }
      console.log('Calling edit endpoint with params:', { 
        model, 
        ...editParams, 
        imageUrls: finalImageUrls.length,
        hasLogo: !!logoImageUrl 
      })
      result = await callEditEndpoint(model, editParams)
    } else {
      // Use text-to-image endpoints when no images
      const textParams = {
        prompt: enhancedPrompt,
        aspectRatio: mappedAspectRatio,
        numImages
      }
      console.log('Calling text-to-image endpoint with params:', { model, ...textParams })
      result = await callTextToImageEndpoint(model, textParams)
    }

    // Log the full response structure for debugging
    console.log('ðŸ” Fal.ai response structure:', {
      hasData: !!result.data,
      hasImages: !!result.data?.images,
      imagesCount: result.data?.images?.length || 0,
      requestId: result.requestId
    })

    // Extract images from the correct response structure
    const images = result.data?.images?.map((img: any) => img.url) || []
    
    // Validate that we actually got images
    if (images.length === 0) {
      console.warn('âš ï¸ Fal.ai returned no images despite successful response')
      return {
        success: false,
        images: [],
        error: 'No images were generated by fal.ai'
      }
    }

    console.log('âœ… Successfully extracted', images.length, 'image URLs from fal.ai response')
    
    return {
      success: true,
      images,
      requestId: result.requestId
    }

  } catch (error) {
    console.error("Fal.ai generation error:", error)
    
    // Log detailed error information for debugging
    if (error && typeof error === 'object' && 'body' in error) {
      console.error("FAL AI error details:", JSON.stringify(error.body, null, 2))
    }
    
    // Enhanced error handling for specific fal.ai errors
    let errorMessage = "Unknown error occurred"
    
    if (error instanceof Error) {
      errorMessage = error.message
      
      // Handle specific fal.ai error types
      if (error.message.includes('ValidationError') || error.message.includes('Unprocessable Entity')) {
        if (error && typeof error === 'object' && 'body' in error) {
          const body = error.body as any
          if (body?.detail) {
            const details = Array.isArray(body.detail) ? body.detail : [body.detail]
            
            // Check for image load errors
            const imageLoadErrors = details.filter((d: any) => d.type === 'image_load_error')
            if (imageLoadErrors.length > 0) {
              errorMessage = 'Failed to load reference images. Please ensure images are valid and try again.'
            }
            
            // Check for other validation errors
            const validationErrors = details.filter((d: any) => d.type !== 'image_load_error')
            if (validationErrors.length > 0) {
              const validationMessages = validationErrors.map((d: any) => d.msg || d.message).join(', ')
              errorMessage = `Validation error: ${validationMessages}`
            }
          }
        }
      }
    }
    
    return {
      success: false,
      images: [],
      error: errorMessage
    }
  }
}

/**
 * Call text-to-image endpoint for the specified model
 */
async function callTextToImageEndpoint(model: FalModel, params: any) {
  const { prompt, aspectRatio, numImages } = params

  switch (model) {
    case "Nano-banana":
      return await getFalClient().subscribe("fal-ai/nano-banana", {
        input: {
          prompt,
          num_images: numImages,
          output_format: "jpeg",
          aspect_ratio: aspectRatio
        }
      })

    case "gpt-image-1":
      return await getFalClient().subscribe("fal-ai/gpt-image-1/text-to-image/byok", {
        input: {
          prompt,
          image_size: aspectRatio,
          num_images: numImages,
          quality: "auto",
          background: "auto",
          openai_api_key: process.env.OPENAI_API_KEY
        }
      })

    case "seedream-v4":
      return await getFalClient().subscribe("fal-ai/bytedance/seedream/v4/text-to-image", {
        input: {
          prompt,
          image_size: aspectRatio,
          num_images: numImages,
          max_images: numImages,
          enable_safety_checker: true
        }
      })

    default:
      throw new Error(`Unsupported model: ${model}`)
  }
}

/**
 * Call edit endpoint for the specified model
 */
async function callEditEndpoint(model: FalModel, params: any) {
  const { prompt, imageUrls, aspectRatio, numImages } = params

  switch (model) {
    case "Nano-banana":
      return await getFalClient().subscribe("fal-ai/nano-banana/edit", {
        input: {
          prompt,
          image_urls: imageUrls,
          num_images: numImages,
          output_format: "jpeg",
          aspect_ratio: aspectRatio
        }
      })

    case "gpt-image-1":
      return await getFalClient().subscribe("fal-ai/gpt-image-1/edit-image/byok", {
        input: {
          prompt,
          image_urls: imageUrls,
          image_size: aspectRatio,
          num_images: numImages,
          quality: "auto",
          input_fidelity: "low",
          openai_api_key: process.env.OPENAI_API_KEY
        }
      })

    case "seedream-v4":
      return await getFalClient().subscribe("fal-ai/bytedance/seedream/v4/edit", {
        input: {
          prompt,
          image_urls: imageUrls,
          image_size: aspectRatio,
          num_images: numImages,
          max_images: numImages,
          enable_safety_checker: true
        }
      })

    default:
      throw new Error(`Unsupported model: ${model}`)
  }
}

/**
 * Download image from URL and return as buffer
 */
export async function downloadImage(url: string): Promise<Buffer> {
  const response = await fetch(url)
  if (!response.ok) {
    throw new Error(`Failed to download image: ${response.statusText}`)
  }
  const arrayBuffer = await response.arrayBuffer()
  return Buffer.from(arrayBuffer)
}

